{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XDFrMKc30565"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import nltk\n",
        "from nltk.corpus import stopwords, names\n",
        "import string\n",
        "import joblib\n",
        "import scipy as sp\n",
        "import textwrap\n",
        "import csv\n",
        "import random\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models, Input\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import  classification_report\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_url = \"https://raw.githubusercontent.com/JULIELab/EmoBank/master/corpus/emobank.csv\"\n",
        "df_data = pd.read_csv(dataset_url,quoting = 0)\n",
        "corpus =  df_data.iloc[:, [5]]\n",
        "vad = df_data.iloc[:, [2,3,4]]"
      ],
      "metadata": {
        "id": "1mpcC2vKgqsF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_file = open('/content/intents.json').read()\n",
        "questions = json.loads(data_file)"
      ],
      "metadata": {
        "id": "V6JsAgJw1Sbr"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "tf_id_array = vectorizer.fit_transform(corpus['text'].tolist()).toarray()"
      ],
      "metadata": {
        "id": "OLjhkm3I2V5b"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('names')\n",
        "\n",
        "from nltk.corpus import stopwords, names\n",
        "import string\n",
        "import re\n",
        "my_stopwords = set(stopwords.words(\"english\") + names.words() + [\"''\", \"'d\", \"'ll\", \"'re\", \"'ve\", '--', '...', '1', '10', '2', '20', '3', '4', '``', '–', '—', '’', '“', '”'])"
      ],
      "metadata": {
        "id": "h053iRqG2Yu9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee210e22-cac0-4893-d84d-9b0845fdad45"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package names to /root/nltk_data...\n",
            "[nltk_data]   Package names is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def thorough_filter(words):\n",
        "    filtered_words = []\n",
        "    regexp = re.compile('[0-9]+')\n",
        "    for word in words:\n",
        "      if not regexp.search(word):\n",
        "        pun = []\n",
        "        for letter in word:\n",
        "          pun.append(letter in string.punctuation.strip('!').strip('?'))\n",
        "        if not all(pun):\n",
        "          filtered_words.append(word)\n",
        "    return filtered_words\n",
        "\n",
        "filtered = thorough_filter(vectorizer.get_feature_names_out())"
      ],
      "metadata": {
        "id": "S0WU1vKQ2cyv"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('rslp')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tc8IeddS5hm2",
        "outputId": "c0177c15-26c2-4d1f-df0c-482771947540"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]   Unzipping stemmers/rslp.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "class SnowballTokenizer(object):\n",
        "    def __call__(self, text):\n",
        "        words = [word for word in nltk.word_tokenize(text) if (word not in my_stopwords)]\n",
        "        words = thorough_filter(words)\n",
        "        return [SnowballStemmer(\"english\").stem(word) for word in words]\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(tokenizer=SnowballTokenizer(), max_df=0.95, min_df=3, sublinear_tf=True)\n",
        "tf_id_array = vectorizer.fit_transform(corpus['text'].tolist()).toarray()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYFkDKq67GCO",
        "outputId": "e07d6d5b-0a62-42d9-ffe7-f5338ebdb48e"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "splits = np.array(df_data.iloc[:,1].tolist())\n",
        "train_set = tf_id_array[splits=='train']\n",
        "train_labels = vad[splits=='train'].to_numpy()\n",
        "test_set = tf_id_array[splits=='test']\n",
        "test_labels = vad[splits=='test'].to_numpy()\n",
        "dev_set = tf_id_array[splits=='dev']\n",
        "dev_labels = vad[splits=='dev'].to_numpy()\n",
        "\n",
        "sample_length = train_set.shape[1]"
      ],
      "metadata": {
        "id": "ECICR_swhYuc"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emotion_coeff = {\n",
        "  'anger': np.array([-0.43, 0.67, 0.34]),\n",
        "  'joy': np.array([0.76, 0.48, 0.35]),\n",
        "  'surpise': np.array([0.4, 0.67, -0.13]),\n",
        "  'disgust': np.array([-0.6, 0.35, 0.11]),\n",
        "  'fear': np.array([-0.64, 0.6, -0.43]),\n",
        "  'sadness': np.array([-0.63, 0.27, -0.33])\n",
        "}\n",
        "\n",
        "vad2emotion = {\n",
        "    0: 'anger',\n",
        "    1: 'joy',\n",
        "    2: 'surprise',\n",
        "    3: 'disgust',\n",
        "    3: 'fear',\n",
        "    4: 'sadness'\n",
        "}"
      ],
      "metadata": {
        "id": "8p6Wi5j1heio"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k,v in emotion_coeff.items():\n",
        "  emotion_coeff[k] = 2*v+3\n",
        "\n",
        "emo_coeff = np.array(list(emotion_coeff.values()))\n",
        "\n",
        "neigh = NearestNeighbors(n_neighbors=1)\n",
        "neigh.fit(emo_coeff)\n",
        "\n",
        "train_labels_emo_dst, train_labels_emo = neigh.kneighbors(train_labels)\n",
        "test_labels_emo_dst, test_labels_emo = neigh.kneighbors(test_labels)"
      ],
      "metadata": {
        "id": "ZFFl7AszhiyQ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_diagnostics(history):\n",
        "\tplt.figure(figsize=(8, 8))\n",
        "\tplt.suptitle('Training Curves')\n",
        "\t# plot loss\n",
        "\tplt.subplot(211)\n",
        "\tplt.title('Loss')\n",
        "\tplt.plot(history.history['loss'], color='blue', label='train')\n",
        "\tplt.plot(history.history['val_loss'], color='orange', label='val')\n",
        "\tplt.legend(loc='upper right')\n",
        "\t# plot accuracy\n",
        "\tplt.subplot(212)\n",
        "\tplt.title('Accuracy')\n",
        "\tplt.plot(history.history['accuracy'], color='blue', label='train')\n",
        "\tplt.plot(history.history['val_accuracy'], color='orange', label='val')\n",
        "\tplt.legend(loc='lower right')\n",
        "\treturn plt"
      ],
      "metadata": {
        "id": "4bz6zpKVhmZe"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_evaluation(model, evaluation_steps, test_set, test_labels):\n",
        "  print('\\nTest set evaluation metrics')\n",
        "  print(model.evaluate(test_set, steps = evaluation_steps))\n",
        "\n",
        "def model_report(model, history, test_set, test_labels, evaluation_steps = 10):\n",
        "  plt = summarize_diagnostics(history)\n",
        "  plt.show()\n",
        "  model_evaluation(model, evaluation_steps, test_set, test_labels)\n",
        "\n",
        "def train_model(model, train_set, train_labels, val_set, val_labels, epochs = 10, steps_per_epoch = 2, validation_steps = 1, tensorboard_entry = False, early_stopping = False, learning_rate = False):\n",
        "\n",
        "  log_dir = \"./logs/fit/\" + model.name + \"/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "  callbacks = []\n",
        "  if tensorboard_entry:\n",
        "    callbacks.append(tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1))\n",
        "  if early_stopping:\n",
        "    callbacks.append(tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta = 0 , patience = 40, restore_best_weights= True))\n",
        "  if learning_rate:\n",
        "      callbacks.append(tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
        "                              patience=10, min_lr=0.00001))\n",
        "\n",
        "  history = model.fit(train_set, train_labels, epochs=epochs, steps_per_epoch=steps_per_epoch, validation_data=(val_set, val_labels), validation_steps=validation_steps, callbacks = callbacks)\n",
        "  return(history)"
      ],
      "metadata": {
        "id": "3Rkzn9tjhpAV"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load = True\n",
        "\n",
        "def init_zuccbot():\n",
        "\n",
        "  model = models.Sequential(name=\"zuccbot\")\n",
        "  model.add(layers.Dense(64, activation='relu'))\n",
        "  model.add(layers.Dense(128, activation='relu'))\n",
        "  model.add(layers.Dense(64, activation='relu'))\n",
        "  model.add(layers.Dense(32, activation='relu'))\n",
        "  model.add(layers.Dense(16, activation='relu'))\n",
        "  model.add(layers.Dense(3))\n",
        "\n",
        "  model.compile(optimizer=\"rmsprop\", loss='mean_squared_error', metrics=['accuracy'])\n",
        "\n",
        "  return model\n",
        "\n",
        "if not load:\n",
        "  model = init_zuccbot()\n",
        "  history = train_model(model, train_set, train_labels, dev_set, dev_labels, 300, 30, 5)\n",
        "  model.save(\"chatbob-model\")\n",
        "else:\n",
        "  model = init_zuccbot()\n",
        "  model = models.load_model(\"chatbob-model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lF_fLbByhu9Y",
        "outputId": "b15a686e-6a3f-4b04-ddc9-af4786e9db33"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if load:\n",
        "  reconstructed_model = models.load_model(\"/content/chatbob-model\")\n",
        "\n",
        "def analyseSentence(sentence):\n",
        "  inp = vectorizer.transform([sentence]).toarray()\n",
        "  vad_pred = model.predict(inp)\n",
        "  emo_dst, emo_pred = neigh.kneighbors(vad_pred, 3)\n",
        "  emos = [vad2emotion[x] for x in emo_pred.ravel()]\n",
        "  return 1/emo_dst.ravel(), emos\n",
        "\n",
        "def analyseQuestionnaire(answers):\n",
        "  emotions = ['anger', 'joy', 'surprise', 'fear', 'sadness']\n",
        "  emo_vals = {x:0 for x in emotions}\n",
        "  for sen in answers:\n",
        "    emo_ints, emos = analyseSentence(sen)\n",
        "    for emo, emo_int in zip(emos, emo_ints):\n",
        "      emo_vals[emo] += emo_int\n",
        "  emo_vals = { emo: emo_int/sum(emo_vals.values()) for emo, emo_int in emo_vals.items()}\n",
        "  return emo_vals"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKa97fGaiVlJ",
        "outputId": "a73c4544-cfb2-4ebf-937f-6b3c337d5c54"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('names')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('rslp')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-b7ZiFIl8qv",
        "outputId": "fb6f4503-cc7e-4c2b-911e-6c79a6d5a08a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package names to /root/nltk_data...\n",
            "[nltk_data]   Package names is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]   Package rslp is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Hello, how are you\"\n",
        "emotion_intensity, emotions = analyseSentence(sentence)\n",
        "print(f\"Emotion Intensity: {emotion_intensity}, Emotions: {emotions}\")\n",
        "\n",
        "questionnaire_answers = [\"You are so dumb, get out!\"]\n",
        "questionnaire_emotions = analyseQuestionnaire(questionnaire_answers)\n",
        "print(\"Emotion Distribution in Questionnaire:\")\n",
        "for emotion, intensity in questionnaire_emotions.items():\n",
        "    print(f\"{emotion}: {intensity}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyKCcF_LmA3I",
        "outputId": "4859dd12-424a-40b1-ba5a-050ec2b9fd3b"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 70ms/step\n",
            "Emotion Intensity: [0.70068466 0.58897946 0.58320214], Emotions: ['sadness', 'anger', 'surprise']\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "Emotion Distribution in Questionnaire:\n",
            "anger: 0.548051637807054\n",
            "joy: 0.0\n",
            "surprise: 0.0\n",
            "fear: 0.20719231603082588\n",
            "sadness: 0.24475604616212002\n"
          ]
        }
      ]
    }
  ]
}